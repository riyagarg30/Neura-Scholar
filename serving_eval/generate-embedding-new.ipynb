{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db674340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /home/pb/.pyenv/versions/3.12.10/envs/jupyter-cuda/lib/python3.12/site-packages (2.9.10)\n",
      "Requirement already satisfied: requests in /home/pb/.pyenv/versions/3.12.10/envs/jupyter-cuda/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/pb/.pyenv/versions/3.12.10/envs/jupyter-cuda/lib/python3.12/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/pb/.pyenv/versions/3.12.10/envs/jupyter-cuda/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/pb/.pyenv/versions/3.12.10/envs/jupyter-cuda/lib/python3.12/site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/pb/.pyenv/versions/3.12.10/envs/jupyter-cuda/lib/python3.12/site-packages (from requests) (2025.4.26)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install psycopg2-binary requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e6a1b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2: Imports & cleanup unused\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import requests\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c329315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 3: Configuration\n",
    "BATCH_SIZE      = 128\n",
    "NUM_WORKERS     = 4\n",
    "DATABASE_URI    = 'postgresql+psycopg2://rg5073:rg5073pass@129.114.27.112:5432/cleaned_meta_data_db'\n",
    "REMOTE_EMBED_URL = 'http://localhost:8000/embed'   # your hosted FastAPI batch-embed URL\n",
    "\n",
    "MODEL_DETAILS = [\n",
    "    {\n",
    "        \"column\": \"chunk_embedding_768\",\n",
    "        \"model_path\": \"/home/pb/projects/course/sem2/mlops/project/mlops/models/distilbert.onnx\",\n",
    "    },\n",
    "    {\n",
    "        \"column\": \"chunk_embedding_768_dyn\",\n",
    "        \"model_path\": \"/home/pb/projects/course/sem2/mlops/project/mlops/models/distilbert_dyn.onnx\",\n",
    "    },\n",
    "    {\n",
    "        \"column\": \"chunk_embedding_768_graph\",\n",
    "        \"model_path\": \"/home/pb/projects/course/sem2/mlops/project/mlops/models/distilbert_opt.onnx\",\n",
    "    },\n",
    "    {\n",
    "        \"column\": \"chunk_embedding_768_static_h\",\n",
    "        \"model_path\": \"/home/pb/projects/course/sem2/mlops/project/mlops/models/distilbert_static_heavy.onnx\",\n",
    "    },\n",
    "    {\n",
    "        \"column\": \"chunk_embedding_768_static_m\",\n",
    "        \"model_path\": \"/home/pb/projects/course/sem2/mlops/project/mlops/models/distilbert_static_moderate.onnx\",\n",
    "    },\n",
    "]\n",
    "\n",
    "column = MODEL_DETAILS[3][\"column\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db815f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4: Ensure the vector column exists\n",
    "engine = create_engine(DATABASE_URI, pool_size=8, max_overflow=0)\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(f\"\"\"\n",
    "      ALTER TABLE arxiv_chunks_eval_5\n",
    "      ADD COLUMN IF NOT EXISTS {column} vector(768)\n",
    "    \"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9ae38b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows to embed: 52554\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 5: Count total rows\n",
    "with engine.connect() as conn:\n",
    "    total = conn.execute(text(\"SELECT COUNT(*) FROM arxiv_chunks_eval_5\")).scalar_one()\n",
    "print(f\"Total rows to embed: {total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dc1e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 6: Remote embedding via batch-embed endpoint\n",
    "def embed_texts(texts: list[str]) -> list[list[float]]:\n",
    "    \"\"\"\n",
    "    Send a list of texts to the remote batch-embed endpoint,\n",
    "    receive back list-of-list embeddings.\n",
    "    \"\"\"\n",
    "    resp = requests.post(REMOTE_EMBED_URL, json={\"texts\": texts})\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()[\"embeddings\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d09a5d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 7: Fetch, embed, and update one batch\n",
    "def process_batch(offset: int) -> int:\n",
    "    # 1) fetch batch\n",
    "    with engine.connect() as conn:\n",
    "        rows = conn.execute(\n",
    "            text(\"\"\"\n",
    "              SELECT paper_id, chunk_id, chunk_data\n",
    "                FROM arxiv_chunks_eval_5\n",
    "               ORDER BY paper_id, chunk_id\n",
    "               LIMIT :limit OFFSET :offset\n",
    "            \"\"\"),\n",
    "            {\"limit\": BATCH_SIZE, \"offset\": offset}\n",
    "        ).fetchall()\n",
    "    if not rows:\n",
    "        return 0\n",
    "\n",
    "    # 2) compute embeddings remotely\n",
    "    ids   = [(r.paper_id, r.chunk_id) for r in rows]\n",
    "    texts = [r.chunk_data for r in rows]\n",
    "    embs  = embed_texts(texts)  # calls your FastAPI\n",
    "\n",
    "    # 3) bulk update back into Postgres\n",
    "    params = [\n",
    "        {\"pid\": pid, \"cid\": cid, \"vec\": vec}\n",
    "        for (pid, cid), vec in zip(ids, embs)\n",
    "    ]\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(\n",
    "            text(f\"\"\"\n",
    "              UPDATE arxiv_chunks_eval_5\n",
    "                 SET {column} = :vec\n",
    "               WHERE paper_id = :pid\n",
    "                 AND chunk_id   = :cid\n",
    "            \"\"\"),\n",
    "            params\n",
    "        )\n",
    "\n",
    "    return len(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d9acc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "822 batches, offsets: [0, 64, 128, 192, 256]‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 8: Compute all offsets\n",
    "n_batches = math.ceil(total / BATCH_SIZE)\n",
    "offsets   = [i * BATCH_SIZE for i in range(n_batches)]\n",
    "print(f\"{n_batches} batches, offsets: {offsets[:5]}‚Ä¶\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cc980a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc4e5197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:15:01 INFO Prepared 3 sample texts for embedding test\n",
      "14:15:01 INFO Sending request to remote /embed endpoint\n",
      "14:15:02 INFO Received embeddings in 1.467 seconds\n",
      "14:15:02 INFO Each embedding has dimension: 768\n",
      "14:15:02 INFO  Sample 0: first 5 dims = [0.12146452069282532, 0.29932883381843567, 0.10192757844924927, 0.24048078060150146, -0.17713797092437744], L2 norm = 9.5468\n",
      "14:15:02 INFO  Sample 1: first 5 dims = [-0.16278865933418274, 0.1646127998828888, 0.6065582036972046, 0.05404533073306084, 0.0028536012396216393], L2 norm = 10.0426\n",
      "14:15:02 INFO  Sample 2: first 5 dims = [-0.21152034401893616, -0.046315282583236694, 0.8236836791038513, -0.011421268805861473, -0.44301533699035645], L2 norm = 10.6453\n"
     ]
    }
   ],
   "source": [
    "# %% Cell: Sample Test with Detailed Logging\n",
    "sample_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"OpenAI's GPT models are powerful for NLP tasks.\",\n",
    "    \"FastAPI + ONNX Runtime is great for serving ML models!\"\n",
    "]\n",
    "\n",
    "logging.info(f\"Prepared {len(sample_texts)} sample texts for embedding test\")\n",
    "\n",
    "# Measure round-trip time for the remote call\n",
    "t0 = time.time()\n",
    "logging.info(\"Sending request to remote /embed endpoint\")\n",
    "embs = embed_texts(sample_texts)\n",
    "t1 = time.time()\n",
    "\n",
    "elapsed = t1 - t0\n",
    "logging.info(f\"Received embeddings in {elapsed:.3f} seconds\")\n",
    "\n",
    "# Validate and inspect embeddings\n",
    "assert isinstance(embs, list) and len(embs) == len(sample_texts), \"Unexpected response format\"\n",
    "dim = len(embs[0])\n",
    "logging.info(f\"Each embedding has dimension: {dim}\")\n",
    "\n",
    "for i, vec in enumerate(embs):\n",
    "    norm = np.linalg.norm(vec)\n",
    "    logging.info(f\" Sample {i}: first 5 dims = {vec[:5]}, L2 norm = {norm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "659274a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import random\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# # Suppose `offsets` is your full list of chunk-offsets.\n",
    "# # We‚Äôll sample 200 of them for each benchmark.\n",
    "# MAX_QUERIES = 20\n",
    "\n",
    "# def benchmark(batch_size, num_workers, offsets):\n",
    "#     # pick 200 at random, or the first 200 if you prefer determinism\n",
    "#     sample = random.sample(offsets, min(MAX_QUERIES, len(offsets)))\n",
    "#     logging.info(f\"Benchmarking with batch_size={batch_size}, num_workers={num_workers}\")\n",
    "#     logging.info(f\"{len(sample)} offsets to process\")\n",
    "#     start = time.perf_counter()\n",
    "#     processed = 0\n",
    "\n",
    "#     with ThreadPoolExecutor(max_workers=num_workers) as exe:\n",
    "#         futures = [exe.submit(process_batch, off) for off in sample]\n",
    "#         for f in as_completed(futures):\n",
    "#             processed += f.result()\n",
    "#             logging.info(f\"Processed {processed} batches\")\n",
    "\n",
    "#     elapsed = time.perf_counter() - start\n",
    "#     tput = processed / elapsed\n",
    "#     return tput\n",
    "\n",
    "# # Grid‚Äêsearching batch_size √ó num_workers\n",
    "# best = (0, None, None)\n",
    "# for bs in [16, 32]:\n",
    "#     for nw in [8]:\n",
    "#         tput = benchmark(bs, nw, offsets)\n",
    "#         logging.info(f\"batch={bs:<3} workers={nw:<2} ‚Üí {tput:.1f} items/sec\")\n",
    "#         if tput > best[0]:\n",
    "#             best = (tput, bs, nw)\n",
    "\n",
    "# logging.info(f\"\\nüèÜ Best throughput: {best[0]:.1f} items/sec \"\n",
    "#       f\"with batch_size={best[1]} and num_workers={best[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62d14fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Ä¶ done 256/52554 rows\r"
     ]
    }
   ],
   "source": [
    "# %% Cell 9: Run batches in parallel and report progress\n",
    "processed = 0\n",
    "with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "    futures = {executor.submit(process_batch, off): off for off in offsets}\n",
    "    for fut in as_completed(futures):\n",
    "        done = fut.result()\n",
    "        processed += done\n",
    "        print(f\"‚Ä¶ done {processed}/{total} rows\", end=\"\\r\")\n",
    "\n",
    "print(f\"\\n‚úÖ Finished embedding & updating {processed} rows\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
